//--------------------------------------------------------------------------
// Code generated by the SmartSoft MDSD Toolchain
// The SmartSoft Toolchain has been developed by:
//  
// Service Robotics Research Center
// University of Applied Sciences Ulm
// Prittwitzstr. 10
// 89075 Ulm (Germany)
//
// Information about the SmartSoft MDSD Toolchain is available at:
// www.servicerobotik-ulm.de
//
// This file is generated once. Modify this file to your needs. 
// If you want the toolchain to re-generate this file, please 
// delete it before running the code generator.
//--------------------------------------------------------------------------
// --------------------------------------------------------------------------
//
//  Copyright (C) 2009-2017 Andreas Steck, Matthias Lutz
//                     2021 Nayabrasul Shaik
//
//        lutz@hs-ulm.de
//        shaik@hs-ulm.de
//
//        ZAFH Servicerobotik Ulm
//        University of Applied Sciences
//        Prittwitzstr. 10
//        D-89075 Ulm
//        Germany
//
//  This library is free software; you can redistribute it and/or
//  modify it under the terms of the GNU Lesser General Public
//  License as published by the Free Software Foundation; either
//  version 2.1 of the License, or (at your option) any later version.
//
//  This library is distributed in the hope that it will be useful,
//  but WITHOUT ANY WARRANTY; without even the implied warranty of
//  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
//  Lesser General Public License for more details.
//
//  You should have received a copy of the GNU Lesser General Public
//  License along with this library; if not, write to the Free Software
//  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
//
// --------------------------------------------------------------------------
#include "TrackingThread.hh"
#include "ComponentRealSensePersonTracker.hh"

#include "CommBasicObjects/CommPose3d.hh"
#include <mrpt/poses/CPose3D.h>
#include <mrpt/poses/CPoint3D.h>
//#include <mrpt/math.h>


#include<vector>
#include <iostream>


TrackingThread::TrackingThread(SmartACE::SmartComponent *comp) 
:	TrackingThreadCore(comp)
{
	std::cout << "constructor TrackingThread\n";

		goalCounter = 0;

		std::string pt_data_files= COMP->getGlobalState().getSettings().getPt_data();

		COMP->ptModule=rs::person_tracking::person_tracking_video_module_factory::create_person_tracking_video_module(GetWC(pt_data_files.c_str()));

		// Enable Person Tracking module
		COMP->ptModule->QueryConfiguration()->QueryTracking()->Enable();
		COMP->ptModule->QueryConfiguration()->QueryRecognition()->Enable();
		COMP->ptModule->QueryConfiguration()->QueryTracking()->SetTrackingMode((Intel::RealSense::PersonTracking::PersonTrackingConfiguration::TrackingConfiguration::TrackingMode)0);

		m_frame_number=0;
		number_of_no_persons_frames=0;

		bool IsShow_depth = COMP->getGlobalState().getSettings().getDisplay_depth_image();

		mVisualization.Init(IsShow_depth);
	    first_image_flag =false;
	    actualModuleConfig = {};

		SmartACE::SmartGuard g(COMP->personLock);
	    COMP->currently_following_person_Id =0; // by default follow the person with Id = 0
	    rs::log_to_console(rs::log_severity::warn);
}
TrackingThread::~TrackingThread() 
{
	std::cout << "destructor TrackingThread\n";
		release_images();
}



int TrackingThread::on_entry()
{
	personFound = false;
	noPersonCount = 0;
	return 0;
}
int TrackingThread::on_execute()
{
	try {
		// wait for scan (PushNewest)
		status = COMP->realSenseClient->getUpdateWait(scan);
		std::chrono::steady_clock::time_point curr_run= std::chrono::steady_clock::now();
		std::cout << "Time difference = " << std::chrono::duration_cast<std::chrono::microseconds>(curr_run - last_run).count() <<std::endl;
		last_run = curr_run;
		//return 0;

		if (status != Smart::SMART_OK) {

			std::cout << "blocking wait  status " << Smart::StatusCodeConversion(status) << " not ok => retry ..." << std::endl;
		}
		else
		{
			//Process RealSense Data
			if(true!=scan.getIs_valid())
			{
				std::cout <<"RealSense Image data is INVALID" << std::endl;
			}else{

				CommBasicObjects::CommPose3d sensor_pose = scan.getSensor_pose();
				CommBasicObjects::CommPose3d base_pose   = scan.getBase_state().get_base_position().get_base_pose3d();

				DomainVision::CommVideoImage comm_rgb_image = scan.getColor_image();
				DomainVision::CommDepthImage comm_depth_image = scan.getDepth_image();

				uint32_t depth_frame_num = comm_depth_image.getSeq_count();
				std::stringstream framenums;
				framenums <<"Frame Number: "<<depth_frame_num;

				//CHS::SmartGuard guard(COMP->rs_tracking_Mutex);
				if(!first_image_flag)
					init_person_tracking();

				//create sample from both images
				m_colorInfo.height = comm_rgb_image.getParameter().height;
				m_colorInfo.width  = comm_rgb_image.getParameter().width;
				m_colorInfo.format = rs::core::pixel_format::rgb8; //TODO convert smartsoft types to realsense types
				m_colorInfo.pitch  = m_colorInfo.width * 3;




				m_depthInfo.height = comm_depth_image.getHeight();
				m_depthInfo.width  = comm_depth_image.getWidth();
				m_depthInfo.format = rs::core::pixel_format::z16; //TODO convert smartsoft types to realsense types
				m_depthInfo.pitch  = m_depthInfo.width * 2;

				cv::Mat rgb_mat(m_colorInfo.height,m_colorInfo.width, CV_8UC3);
				cv::Mat depth_mat(m_depthInfo.height,m_depthInfo.width, CV_16UC1);
				/*---------------------------------------------------------------------------------------------------------------------------------------------*/
				// copy rgb data into mat
				{
					const unsigned char* imageData = comm_rgb_image.get_data();
					for (int r = 0; r < rgb_mat.rows; r++)
					{
						for (int c = 0; c < rgb_mat.cols; c++)
						{
							const unsigned char* pixel = (imageData + r * 3* rgb_mat.cols + c * 3);

							rgb_mat.at<cv::Vec3b>(r,c)[0]=pixel[2];
							rgb_mat.at<cv::Vec3b>(r,c)[1]=pixel[1];
							rgb_mat.at<cv::Vec3b>(r,c)[2]=pixel[0];
						}
					}
				}
				/*---------------------------------------------------------------------------------------------------------------------------------------------*/

				// copy distance data into mat
				{
					const uint16_t* depth_data_uint16;
					const float* depth_data_float;
					DomainVision::DepthFormatType depth_format = comm_depth_image.getFormat();
					if(depth_format==DomainVision::DepthFormatType::UINT16)
					{
						//depth_data_uint16 = comm_depth_image.get_distances_data<const uint16_t*>();
						depth_data_uint16 = comm_depth_image.get_distances_uint16();

					}else if (depth_format==DomainVision::DepthFormatType::FLOAT)
					{
						depth_data_float = comm_depth_image.get_distances_float();

					}

					uint32_t depth_width   =  m_depthInfo.width;
					uint32_t depth_height  =  m_depthInfo.height;

					for (uint32_t depth_row = 0; depth_row < depth_height ; ++depth_row){//along y
						for (uint32_t depth_col = 0; depth_col < depth_width;++depth_col){//along x-axis

							if(depth_format==DomainVision::DepthFormatType::UINT16)
							{
								const uint16_t pixel = *(depth_data_uint16 + depth_row * depth_mat.cols + depth_col);
								depth_mat.at<uint16_t>(depth_row, depth_col)=pixel;
							}else if (depth_format==DomainVision::DepthFormatType::FLOAT)
							{
								const float pixel = *(depth_data_float + depth_row * depth_mat.cols + depth_col);
								depth_mat.at<uint16_t>(depth_row, depth_col)=static_cast<uint16_t>(pixel*1000); // realsense requires uint16_t format with distance in mm
							}
						}
					}
				}
				/*---------------------------------------------------------------------------------------------------------------------------------------------*/

				//sample_set=m_smartsoft2realsense->CreateSdkSampleSet(rgb_mat.data, m_colorInfo,depth_mat.data, m_depthInfo);
				sample_set=m_smartsoft2realsense->CreateSdkSampleSet(rgb_mat.data, m_colorInfo, depth_mat.data, m_depthInfo);
				m_frame_number++;
				// Process frame


				if (COMP->ptModule->process_sample_set(sample_set) != rs::core::status_no_error)
				{
					std::cerr << "Error : Failed to process sample" << std::endl;
				}
				//status = COMP->stateSlave->tryAcquire("follow");
				//if (status == CHS::SMART_OK)
				{
					SendGoal(*COMP->ptModule->QueryOutput());

					CommBasicObjects::CommPose3d sensor_pose = scan.getSensor_pose();

					DisplayTrackingResult(*COMP->ptModule->QueryOutput(),rgb_mat, depth_mat, framenums.str(), sensor_pose);

					release_images();
				}
				//status = COMP->stateSlave->release("follow");
			}// Valid Data in the scan received


		}

	}
		catch (std::exception &e)
		{
			std::cout << "exception: " << e.what() << std::endl;
		}

		return 0;
}
int TrackingThread::on_exit()
{
	// use this method to clean-up resources which are initialized in on_entry() and needs to be freed before the on_execute() can be called again
	return 0;
}
void TrackingThread::init_person_tracking()
{


	DomainVision::CommVideoImage comm_rgb_image = scan.getColor_image();
	DomainVision::CommDepthImage comm_depth_image = scan.getDepth_image();
	// fill color intrinsics from CommRGBDImage
	arma::mat comm_color_intrinsic = arma::zeros(4,4);

	comm_color_intrinsic = comm_rgb_image.get_intrinsic();

	m_colorInt.fx = comm_color_intrinsic(0,0);
	m_colorInt.fy = comm_color_intrinsic(1,1);
	m_colorInt.ppx = comm_color_intrinsic(0,2);
	m_colorInt.ppy = comm_color_intrinsic(1,2);
	m_colorInt.width = comm_rgb_image.getParameter().width;
	m_colorInt.height= comm_rgb_image.getParameter().height;

	DomainVision::ImageDistortionModel color_distortion_model = comm_rgb_image.getDistortion_model();

	if(color_distortion_model == DomainVision::ImageDistortionModel::BROWN_CONRADY)
		m_colorInt.model = rs::core::distortion_type::modified_brown_conrady;
	else
		m_colorInt.model = rs::core::distortion_type::none;

	arma::mat comm_color_distortion = arma::zeros(1,5);

	comm_color_distortion = comm_rgb_image.get_distortion();
	m_colorInt.coeffs[0] = comm_color_distortion(0,0);
	m_colorInt.coeffs[1] = comm_color_distortion(0,1);
	m_colorInt.coeffs[2] = comm_color_distortion(0,2);
	m_colorInt.coeffs[3] = comm_color_distortion(0,3);
	m_colorInt.coeffs[4] = comm_color_distortion(0,4);


	// fill depth intrinsics from CommRGBDImage
	arma::mat comm_depth_intrinsic = arma::zeros(4,4);

	comm_depth_intrinsic = comm_depth_image.get_intrinsic();

	m_depthInt.fx = comm_depth_intrinsic(0,0);
	m_depthInt.fy = comm_depth_intrinsic(1,1);

	m_depthInt.ppx = comm_depth_intrinsic(0,2);
	m_depthInt.ppy = comm_depth_intrinsic(1,2);

	m_depthInt.height = comm_depth_image.getHeight();
	m_depthInt.width  = comm_depth_image.getWidth();

	DomainVision::ImageDistortionModel depth_distortion_model = comm_depth_image.getDistortion_model();

	if(depth_distortion_model == DomainVision::ImageDistortionModel::BROWN_CONRADY)
		m_depthInt.model = rs::core::distortion_type::modified_brown_conrady;
	else
		m_depthInt.model = rs::core::distortion_type::none;


	arma::mat comm_depth_distortion = arma::zeros(1,5);
	comm_depth_distortion = comm_depth_image.get_distortion();
	m_depthInt.coeffs[0] = comm_depth_distortion(0,0);
	m_depthInt.coeffs[1] = comm_depth_distortion(0,1);
	m_depthInt.coeffs[2] = comm_depth_distortion(0,2);
	m_depthInt.coeffs[3] = comm_depth_distortion(0,3);
	m_depthInt.coeffs[4] = comm_depth_distortion(0,4);

	arma::mat comm_depth_extrinsics = arma::zeros(1,12);
	comm_depth_extrinsics = comm_depth_image.get_extrinsic();

	d2c_extrinsics.rotation[0] =     comm_depth_extrinsics(0,0);
	d2c_extrinsics.rotation[1] =     comm_depth_extrinsics(0,1);
	d2c_extrinsics.rotation[2] =     comm_depth_extrinsics(0,2);
	d2c_extrinsics.rotation[3] =     comm_depth_extrinsics(0,3);
	d2c_extrinsics.rotation[4] =     comm_depth_extrinsics(0,4);
	d2c_extrinsics.rotation[5] =     comm_depth_extrinsics(0,5);
	d2c_extrinsics.rotation[6] =     comm_depth_extrinsics(0,6);
	d2c_extrinsics.rotation[7] =     comm_depth_extrinsics(0,7);
	d2c_extrinsics.rotation[8] =     comm_depth_extrinsics(0,8);

	d2c_extrinsics.translation[0] = comm_depth_extrinsics(0,9);
	d2c_extrinsics.translation[1] = comm_depth_extrinsics(0,10);
	d2c_extrinsics.translation[2] = comm_depth_extrinsics(0,11);



	actualModuleConfig=m_smartsoft2realsense->CreateSdkModuleConfig(m_colorInt, m_depthInt , d2c_extrinsics);

	actualModuleConfig.device_info.rotation = rs::core::rotation::rotation_0_degree;

	//setting the selected configuration (after projection)
	rs::core::status st = COMP->ptModule->set_module_config(actualModuleConfig);
	if (st != rs::core::status_no_error)
	{
		std::cerr << "status_no_error" << std::endl;
	}

	realsense_pt_data = COMP->ptModule->QueryOutput();

    // -1 is to track all the people in the frame
	int  id=0;
	realsense_pt_data->StartTracking(id);

	first_image_flag =true;
	std::cout << "Started tracking everyone in the frame" <<std::endl;

}

/**
 * Transforms the inserted point from sensor coordinate system
 * to robot coordinate system.
 */
mrpt::poses::CPoint3D TrackingThread::transormPointToRobotCoord(const mrpt::poses::CPoint3D & point, CommBasicObjects::CommPose3d& sensor_pose) {

	double sensor_yaw = sensor_pose.get_azimuth(), sensor_pitch = sensor_pose.get_elevation(), sensor_roll = sensor_pose.get_roll();
	double sensor_x = sensor_pose.getPosition().getX() / 1000, sensor_y = sensor_pose.getPosition().getY() / 1000, sensor_z = sensor_pose.getPosition().getZ() / 1000;

	mrpt::poses::CPose3D sensorPose(sensor_x, sensor_y, sensor_z,sensor_yaw, sensor_pitch, sensor_roll);
	mrpt::poses::CPoint3D result =  sensorPose + point;

	return result;
}

///**
// * Transforms the inserted point from realsense frame to rgbd camera frame used
// *
// */
//mrpt::poses::CPoint3D TrackingThread::transormPointToRGBDCoord(const mrpt::poses::CPoint3D & point, CommBasicObjects::CommPose3d sensor_pose) {
//
//	double sensor_yaw = sensor_pose.get_azimuth(), sensor_pitch = sensor_pose.get_elevation(), sensor_roll = sensor_pose.get_roll();
//	double sensor_x = sensor_pose.getPosition().getX() / 1000, sensor_y = sensor_pose.getPosition().getY() / 1000, sensor_z = sensor_pose.getPosition().getZ() / 1000;
//
//
//	std::cout << "pose: x "<< sensor_x<< "pose: y "<< sensor_y<< "pose: z "<< sensor_z<<std::endl;
//	std::cout << "pose: yaw "<< sensor_yaw<< "pose: pitch "<< sensor_pitch<< "pose: roll "<< sensor_roll<<std::endl;
//	mrpt::poses::CPose3D sensorPose(sensor_x, sensor_y, sensor_z,sensor_yaw, sensor_pitch, sensor_roll);
//	mrpt::poses::CPoint3D result =  sensorPose + point;
//
//	return result;
//}

void TrackingThread::SendGoal(Intel::RealSense::PersonTracking::PersonTrackingData &mtrackingData)
{
	 //COMP->currently_following_person_Id =0; // testing
	    int number_of_people_in_frame = mtrackingData.QueryNumberOfPeople();

	    //mtrackingData.QueryPersonDataById(req.tracking_id);

	    if(number_of_people_in_frame>0)
	    {
	    	bool person_found = false; // will be true if the person is present in the frame
	    	for (int index = 0; index < number_of_people_in_frame; index++)
	    	{
	    		Intel::RealSense::PersonTracking::PersonTrackingData::Person *personData = nullptr;
	    		personData = mtrackingData.QueryPersonData(Intel::RealSense::PersonTracking::PersonTrackingData::ACCESS_ORDER_BY_INDEX, index);

	    		if (personData) // someone is in the current frame
	    		{
	    			int id = personData->QueryTracking()->QueryId(); // later to check if the same person or not
	    			if(id==COMP->currently_following_person_Id) // person we are following is in the current frame
	    			{
	    			Intel::RealSense::PersonTracking::PersonTrackingData::PointCombined centerMass = personData->QueryTracking()->QueryCenterMass();
	    			Intel::RealSense::PersonTracking::PersonTrackingData::PersonRecognition* recognition = personData->QueryRecognition();

//	    			int32_t outputRecognitionId;
//	    			int32_t outputTrackingId;
//	    			int32_t outDescriptorId;
//
//	    			PersonRecognition::RecognizerData result;
//	    			int status = personData->QueryRecognition()->RecognizeUser(&result);
//	    			if(status == 0) // 0 - recognition success, else failed
//	    			{
//
//	    				std::cout << "Person with Id ="<<id << " recognized with Id ="<< result.recognitionId <<std::endl;
//
//	    			}else
//	    			{
//	    				std::cout << "Person with Id ="<<id << " is not recognized. Status = "<<status<< std::endl;
//	    			}

//	    			auto status = recognition->RegisterUser(&outputRecognitionId, &outputTrackingId, &outDescriptorId);
//	    			if (status == PersonTrackingData::PersonRecognition::RegistrationSuccessful)
//	    			{
//	    				std::cout << "-----------------------------Registered person: " << outputRecognitionId << std::endl;
//	    				int rid = outputRecognitionId;
//	    			}
//	    			else if (status == PersonTrackingData::PersonRecognition::RegistrationFailedAlreadyRegistered)
//	    			{
//	    				int rid = outputRecognitionId;
//	    				std::cout << " Already Registered person: " << outputRecognitionId << std::endl;
//
//	    			}else
//	    			{
//	    				std::cout << " UnknowStatus : " << status << std::endl;
//	    			}

	    			// center of mass point
	    			CommBasicObjects::CommPose3d sensor_pose = scan.getSensor_pose();
	    			mrpt::poses::CPoint3D point_in_realsense_frame(centerMass.world.point.x, centerMass.world.point.y, centerMass.world.point.z);

	    			//mrpt::poses::CPoint3D point_in_rgbd_camera = transormPointToRobotCoord(point_in_realsense_frame, rgbd_pose);
	    			mrpt::poses::CPoint3D point_in_base =transormPointToRobotCoord(point_in_realsense_frame,sensor_pose);

	    			//std::cout << "Point in point_in_realsense_frame: "<< point_in_realsense_frame.x()*1000<<", "<< point_in_realsense_frame.y()*1000<<", "<< point_in_realsense_frame.z()*1000<<std::endl;
	    			//std::cout << "Point in point_in_rgbd_camera:     "<< point_in_rgbd_camera.x()*1000<<", "<< point_in_rgbd_camera.y()*1000<<", "<< point_in_rgbd_camera.z()*1000<<std::endl;
	    			//std::cout << "Point in Robot_base_frame:         "<< point_in_base.x()*1000<<", "<< point_in_base.y()*1000<<", "<< point_in_base.z()*1000<<std::endl;

	    			number_of_no_persons_frames=0;
	    			SendEventPersonFoundLost(CommTrackingObjects::PersonLostEventType::PERSON_FOUND);
	    			SendTrackingGoalToServer(0, 0, point_in_base.x(), point_in_base.y(), true, CommTrackingObjects::TrackingGoalType::XY_ROBOT);// tracking goal in meters
	    			person_found = true;
	    			break; // go out of For-Loop, as we have already found the Data of the person we are currently following
	    			}

	    		}

	    	}
	    	if(person_found == false)
	    		{
	    			++number_of_no_persons_frames;
	    			SendTrackingGoalToServer(0, 0, 0, 0, false, CommTrackingObjects::TrackingGoalType::XY_ROBOT);// send (0,0) as goal with false flag to stop the robot
	    			if(number_of_no_persons_frames>8*1)// if person is lost for more than x frames consecutively
	    			{

	    				SendEventPersonFoundLost(CommTrackingObjects::PersonLostEventType::PERSON_LOST);
	    				std::cout << "-----------------------------------------------------------------------"<<std::endl;
	    				std::cout << "                            Person Lost                                "<<std::endl;
	    				std::cout << "-----------------------------------------------------------------------"<<std::endl;
	    				number_of_no_persons_frames=0;
	    			}

	    		}

	    }
	    else // no one is the frame
	    {
	    	++number_of_no_persons_frames;
	    	SendTrackingGoalToServer(0, 0, 0, 0, false, CommTrackingObjects::TrackingGoalType::XY_ROBOT);// send (0,0) as goal with false flag to stop the robot
	    	if(number_of_no_persons_frames>30)// if person is lost for more than x frames consecutively
	    	{

	    		std::cout << "-----------------------------------------------------------------------"<<std::endl;
	    		std::cout << "                            Person Lost                                "<<std::endl;
	    		std::cout << "-----------------------------------------------------------------------"<<std::endl;
	    		SendEventPersonFoundLost(CommTrackingObjects::PersonLostEventType::PERSON_LOST);
	    		number_of_no_persons_frames=0;
	    	}
	    }



}

void TrackingThread::DisplayTrackingResult(Intel::RealSense::PersonTracking::PersonTrackingData &mtrackingData,
		                                   cv::Mat &rgb, cv::Mat &depth, std::string str_framenum, CommBasicObjects::CommPose3d &sensor_pose)

{

	mVisualization.Reset();

	{
		SmartACE::SmartGuard g(COMP->personLock);
	COMP->set_tracked_persons.clear();

	//realsense_pt_data = ptModule->QueryOutput();
	if(mtrackingData.QueryNumberOfPeople()>0)
	{
		for (int index = 0; index < mtrackingData.QueryNumberOfPeople(); index++)
		{
			Intel::RealSense::PersonTracking::PersonTrackingData::Person *personData = nullptr;
			personData = mtrackingData.QueryPersonData(Intel::RealSense::PersonTracking::PersonTrackingData::ACCESS_ORDER_BY_INDEX, index);


			if (personData)
			{

				Intel::RealSense::PersonTracking::PersonTrackingData::PersonTracking *personTrackingData = personData->QueryTracking();
				Intel::RealSense::PersonTracking::PersonTrackingData::BoundingBox2D box = personTrackingData->Query2DBoundingBox();
				Intel::RealSense::PersonTracking::PersonTrackingData::PointCombined centerMass = personTrackingData->QueryCenterMass();
				Intel::RealSense::PersonTracking::PersonTrackingData::PersonTracking::DetectionSource detectionsource = personTrackingData->QueryDetectionSource();
				int Id=personTrackingData->QueryId();
				TrackedPerson TP;
				TP.realsense_id = Id;
				TP.box.h        = box.rect.h;
				TP.box.w        = box.rect.w;
				TP.box.x        = box.rect.x;
				TP.box.y        = box.rect.y;
				TP.image.x      = centerMass.image.point.x;
				TP.image.y      = centerMass.image.point.y;

				TP.world.x      = centerMass.world.point.x;
				TP.world.y      = centerMass.world.point.y;
				TP.world.z      = centerMass.world.point.z;

				COMP->set_tracked_persons.insert(TP);

				// person rectangle
				cv::Point pt1(box.rect.x, box.rect.y);
				cv::Point pt2(box.rect.x + box.rect.w, box.rect.y + box.rect.h);
				cv::Rect userRectangle(pt1, pt2);

				// center of mass point
				cv::Point PcenterMass(centerMass.image.point.x, centerMass.image.point.y);

				mrpt::poses::CPoint3D point_in_realsense_frame(centerMass.world.point.x, centerMass.world.point.y, centerMass.world.point.z);
				//mrpt::poses::CPoint3D point_in_rgbd_camera = transormPointToRobotCoord(point_in_realsense_frame, rgbd_pose);
				mrpt::poses::CPoint3D point_in_base =transormPointToRobotCoord(point_in_realsense_frame, sensor_pose);
				cv::Point3f centre_of_masss(point_in_base.x(), point_in_base.y(), point_in_base.z());

				if(Id == COMP->currently_following_person_Id)
				mVisualization.DrawPerson(rgb, Id, userRectangle, PcenterMass, centre_of_masss, true);
				else
				mVisualization.DrawPerson(rgb, Id, userRectangle, PcenterMass, centre_of_masss, false);


			}
			else
			{
				std::cout << "QueryPersonData is Empty" <<std::endl;
			}
		}
	}else
	{
		std::stringstream str_no_one;
		str_no_one<<"No one is in the frame";
		mVisualization.DrawLineAtSummaryReport(rgb,str_no_one.str());

	}
	}

	mVisualization.DrawLineAtSummaryReport(rgb,str_framenum);
	mVisualization.ShowFrames(rgb, depth);
	publish_rgb_image(rgb);

}

/**
 * @brief Method to convert char* to wchar_t*?
 * implementation from https://stackoverflow.com/a/8032108/1595504
 */
const wchar_t *TrackingThread::GetWC(const char *c)
{
	const size_t cSize = strlen(c)+1;
	wchar_t* wc = new wchar_t[cSize];
	mbstowcs (wc, c, cSize);

	return wc;
}

/**
 * @brief Release the sample_set after processing.
 */
void TrackingThread::release_images()
{
	for (uint32_t i = 0; i < static_cast<uint8_t>(rs::core::stream_type::max); ++i)
	{
		rs::core::image_interface* image = sample_set.images[i];
		if (image)
		{
			image->release();
		}
	}

}

bool TrackingThread::startStopTracking(bool isStart, int personId)
{
	Intel::RealSense::PersonTracking::PersonTrackingData *trackingData = COMP->ptModule->QueryOutput();
	Intel::RealSense::PersonTracking::PersonTrackingData::Person *personData = trackingData->QueryPersonDataById(personId);
	if (!personData)
	{
		std::cout<<"Couldn't find tracking request target"<<std::endl;
		return false;
	}
	std::cout<<"Found tracking request target"<<std::endl;
	if (isStart)
	{
		std::cout<<"start tracking on person: " << personId<<std::endl;
		trackingData->StartTracking(personId);
	}
	else
	{
		std::cout<<"stop tracking on person: " << personId<<std::endl;
		trackingData->StopTracking(personId);
	}

	return true;
}

void TrackingThread::SendEventPersonFoundLost(CommTrackingObjects::PersonLostEventType current_event)
{
	CommTrackingObjects::PersonLostEventState s;
	s.set(current_event);
	COMP->personLostEventServer->put(s);
}

void TrackingThread::resetGoalCounter(){
	SmartACE::SmartGuard g(goalCounterLock);
	this->goalCounter=0;
}

void TrackingThread::SendTrackingGoalToServer(double _angle, double _distance, double _x, double _y, bool _valid, CommTrackingObjects::TrackingGoalType _type)
{
	CommTrackingObjects::CommTrackingGoal trackingGoal;
	{
		SmartACE::SmartGuard g(goalCounterLock);
	this->goalCounter++;
	trackingGoal.setGoalCount(this->goalCounter);

	std::cout << "counter : " <<trackingGoal.getGoalCount() << std::endl;
	}
	if(_valid == true && _x !=0 && _y !=0)
	{
	trackingGoal.set( _angle, _distance, _x, _y, _valid);
	trackingGoal.setTrackingType(_type);
	COMP->trackingGoalServer->put(trackingGoal);
	std::cout << "RealSense Tracking goal(in m) x: " << _x << "; y: " << _y <<", goaltype : " <<_type<< std::endl;
	}else if(_valid == false && _x ==0 && _y ==0)
	{
	trackingGoal.set( _angle, _distance, _x, _y, _valid);
	trackingGoal.setTrackingType(_type);
	std::cout<<"Sending tracking goal : "<<trackingGoal<<std::endl;
	COMP->trackingGoalServer->put(trackingGoal);
	std::cout << "RealSense Tracking goal(in m) x: " << _x << "; y: " << _y <<", goaltype : " <<_type<< std::endl;
	}
}

void TrackingThread::publish_rgb_image (cv::Mat &rgb)
{
	DomainVision::CommVideoImage comm_rgb_frame;

	comm_rgb_frame.set_parameters(rgb.cols, rgb.rows, DomainVision::FormatType::RGB24);
	comm_rgb_frame.set_data(rgb.data);
	comm_rgb_frame.setIs_valid(true);
	comm_rgb_frame.setSeq_count(0);

	COMP->rgb_video_server->put(comm_rgb_frame);

}
