//--------------------------------------------------------------------------
// Code generated by the SmartSoft MDSD Toolchain
// The SmartSoft Toolchain has been developed by:
//  
// Service Robotics Research Center
// University of Applied Sciences Ulm
// Prittwitzstr. 10
// 89075 Ulm (Germany)
//
// Information about the SmartSoft MDSD Toolchain is available at:
// www.servicerobotik-ulm.de
//
// This file is generated once. Modify this file to your needs. 
// If you want the toolchain to re-generate this file, please 
// delete it before running the code generator.
//--------------------------------------------------------------------------
#include "RackDetectionTask.hh"
#include "ComponentRackDetection.hh"

#include "CommBasicObjects/CommPosition3d.hh"
#include "colors.hh"
#include <mrpt/system/CTicTac.h>

#include <iostream>

#include <Eigen/src/Core/Matrix.h>
#include <Eigen/src/Core/MatrixBase.h>
#include <Eigen/src/Core/DenseBase.h>

#include <pcl/filters/passthrough.h>
#include <pcl/io/pcd_io.h>
#include <pcl/point_types.h>
#include <pcl/registration/icp.h>
#include <pcl/surface/concave_hull.h>
#include <pcl/filters/voxel_grid.h>
#include <pcl/filters/radius_outlier_removal.h>
#include <pcl/common/transforms.h>


#include <cmath>
#include <string>

#define STOP while (std::cin.get() != '\n');

RackDetectionTask::RackDetectionTask(SmartACE::SmartComponent *comp): RackDetectionTaskCore(comp)
{
	std::cout << "constructor RackDetectionTask\n";
	_model_file_path = COMP->getGlobalState().getGeneral().getModel_path();
	_model_file_save_path = COMP->getGlobalState().getModelCreation().getSave_model_path();
	_create_model = COMP->getGlobalState().getModelCreation().getCreate_model();

	_cloud_window_index = vHelper.createPointCloudWindow("Red = current env; Green = model; Blue = ICP result");
	_rgb_window_index = vHelper.createRgbWindow("RGB Image");

	_send_obstacle_mesh = COMP->getGlobalState().getGeneral().getSend_obstacle_mesh();

	_obj_height = COMP->getGlobalState().getGeneral().getObj_height();
	_obj_width = COMP->getGlobalState().getGeneral().getObj_width();
	_obj_depth = COMP->getGlobalState().getGeneral().getObj_depth();

	_max_detection_score = COMP->getGlobalState().getGeneral().getMax_detection_score();

	//TODO: Value should come from sequencer or PTU state client
	_ptu_pan = 0.0; // default camera looks to the front

	std::string obj_types_str = COMP->getGlobalState().getGeneral().getObj_types();

	while (true) {
		std::size_t comma_index = obj_types_str.find(",");
		if (comma_index != std::string::npos) {
			std::string tmp_obj_type = obj_types_str.substr(0, comma_index);
			_obj_types.push_back(tmp_obj_type);
			obj_types_str = obj_types_str.substr(comma_index + 1,
					obj_types_str.length());

			std::cout << "Object type found: " << tmp_obj_type << std::endl;
		} else {
			if (obj_types_str.length() > 0) {
				std::string tmp_obj_type = obj_types_str.substr(0, obj_types_str.length());
				_obj_types.push_back(tmp_obj_type);
				obj_types_str = "";

				std::cout << "Object type found: " << tmp_obj_type << std::endl;
			}else {
				break;
			}
		}
	}
}
RackDetectionTask::~RackDetectionTask() 
{
	std::cout << "destructor RackDetectionTask\n";
}



int RackDetectionTask::on_entry()
{
	// do initialization procedures here, which are called once, each time the task is started
	// it is possible to return != 0 (e.g. when initialization fails) then the task is not executed further
	COMP->environmentIdCounter = 0;
	COMP->objectIdCounter = 0;
	return 0;
}
int RackDetectionTask::on_execute()
{

	COMP->start_recognition.acquire();

	CommBasicObjects::CommVoid v;
	DomainVision::CommRGBDImage rgbd_image;

	// Get new image from Kinect by query
	bool is_valid_image = false;
	Smart::StatusCode status;
	do{
	status = COMP->kinectQueryClient->query(v, rgbd_image);
	is_valid_image = rgbd_image.getIs_valid();

	if(status != Smart::SMART_OK) {
		std::cout << COLOR_RED << "[RackDetectionTask] Error receiving the image: " << Smart::StatusCodeConversion(status) << COLOR_DEFAULT << std::endl;
	}
	if(!is_valid_image) {
		std::cout << COLOR_RED << "[RackDetectionTask] Invalid Image! retry..." << COLOR_DEFAULT << std::endl;
	}
	usleep(1000000);
	}while (!is_valid_image);

	DomainVision::CommVideoImage current_video_image;
	DomainVision::CommDepthImage current_depth_image;

	current_video_image = rgbd_image.getColor_image();
	current_depth_image = rgbd_image.getDepth_image();

	_pointManipulator = PointManipulation(rgbd_image.getSensor_pose(), current_depth_image.getIntrinsic_mCopy(), current_video_image.getIntrinsic_mCopy(), &current_depth_image);

	vHelper.showRgbImage(_rgb_window_index, current_video_image.get_data(), current_video_image.get_width(), current_video_image.get_height());


	std::cout << "Creating environment point cloud..." << std::flush;

	pcl::PointCloud<pcl::PointXYZ>::Ptr env_cloud (new pcl::PointCloud<pcl::PointXYZ>());
	_pointManipulator.createPointCloud(env_cloud, true);
	std::cout << " Done!" << std::endl;
	std::cout << "Env cloud size: " << env_cloud->size() << std::endl;

	//findPickDirection(env_cloud);
	CommBasicObjects::CommPose3d sensor_pose = rgbd_image.getSensor_pose();
	mrpt::poses::CPose3D sensor_pose_mrpt(sensor_pose.get_x(), sensor_pose.get_y(), sensor_pose.get_z(), sensor_pose.get_azimuth(), sensor_pose.get_elevation(), sensor_pose.get_roll());
	findPickDirection(sensor_pose_mrpt);


	std::cout << "Distance filter environment cloud..." << std::flush;
	env_cloud = cutOutRack(env_cloud);
	std::cout << " Done!" << std::endl;
	std::cout << "Environment cloud size: " << env_cloud->size() << std::endl;


	std::cout << "Voxel filter environment cloud..." << std::flush;
	env_cloud = downsampleCloud(env_cloud);
	std::cout << " Done!" << std::endl;
	std::cout << "Environment cloud size: " << env_cloud->size() << std::endl;


	if (_create_model) {
		std::cout << "Creating rack model..." << std::flush;
		createRackModelCloud (env_cloud);
		std::cout << " Done!" << std::endl;
		std::cout << "Saved to: " << _model_file_save_path << std::endl;
	}

	std::cout << "Loading Rack model..." << std::flush;
	pcl::PointCloud<pcl::PointXYZ>::Ptr rack_model_cloud = loadRackModel(_model_file_path);
	std::cout << " Done!" << std::endl;
	std::cout << "Model cloud size: " << rack_model_cloud->size() << std::endl;


	if(rack_model_cloud->size() > 50000){
		std::cout << "Voxel filter model cloud..." << std::flush;
		rack_model_cloud = downsampleCloud(rack_model_cloud);
		std::cout << " Done!" << std::endl;
		std::cout << "Model cloud size: " << rack_model_cloud->size() << std::endl;
	}

	Eigen::Matrix<float, 4, 4> transf_mat = doIcp(env_cloud, rack_model_cloud);

	Eigen::Matrix<float, 4, 4> zero_mat;
	zero_mat << 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0;


	setEnvId();

	//Is transformation matrix is empty (rack not found)?
	if(transf_mat.isApprox(zero_mat)){
		setGrippableObjectsEpmty();
	}else{

		mrpt::poses::CPose3D rack_pose = getRackPoseFromTransfMat(transf_mat);
		std::cout << "Rack pose:" << rack_pose << std::endl;

		setGrippableObjects(rack_pose);

		std::cout << "Cut objects out of environment..." << std::flush;
		env_cloud = cutOutObjects(env_cloud);
		std::cout << " Done!" << std::endl;
		std::cout << "Environment cloud size: " << env_cloud->size() << std::endl;
	}

	if(_send_obstacle_mesh){
		setObstacleObject(env_cloud);
	}
	sendFinishedEvent();

	for (std::list<ConcreteObject>::const_iterator iter = COMP->concreteObjects.begin(); iter != COMP->concreteObjects.end(); iter++) {
		std::cout << "  ID:" << iter->getId() << std::endl;
		std::cout << "  Surface Pose:" << iter->getSurfacePose() << std::endl;
		std::cout << "  Object center Pose:" << iter->getPose() << std::endl;
		std::cout << "  Object type: " << iter->getObjectClass() << std::endl;
		std::cout << "  ---------" << std::endl;

		mrpt::poses::CPose3D tmp_pose(iter->getSurfacePose().x(), iter->getSurfacePose().y(), iter->getSurfacePose().z(), iter->getSurfacePose().yaw(), iter->getSurfacePose().pitch(), iter->getSurfacePose().roll());
		vHelper.addSmallCoordinateFrame(0, tmp_pose);
	}

	// it is possible to return != 0 (e.g. when the task detects errors), then the outer loop breaks and the task stops
	return 0;
}
int RackDetectionTask::on_exit()
{
	// use this method to clean-up resources which are initialized in on_entry() and needs to be freed before the on_execute() can be called again
	return 0;
}

/*
 * Sends an finished event with empty object list
 */
void RackDetectionTask::errorQuit(){
	//sendFinishedEvent();
}

void RackDetectionTask::setEnvId() {
//TODO parameters are missing
//	std::cout << "setting env id: " << COMP->getGlobalState().getCommObjectRecognitionObjects().getObjectRecognitionParameter().getSETENVID().getId() << std::endl;
//	COMP->environmentIdCounter = COMP->getGlobalState().getCommObjectRecognitionObjects().getObjectRecognitionParameter().getSETENVID().getId();

//	std::cout << "setting obj id: " << COMP->getGlobalState().getCommObjectRecognitionObjects().getObjectRecognitionParameter().getSETENVID().getId() << std::endl;
//	COMP->objectIdCounter = COMP->getGlobalState().getCommObjectRecognitionObjects().getObjectRecognitionParameter().getSETOBJECTID().getId();
}


/*
 * Prepares and sets an event for signaling the completion of detection
 */
void RackDetectionTask::sendFinishedEvent(){
	CommObjectRecognitionObjects::CommObjectRecognitionEventState event;

	event.set_environment_id(COMP->environmentIdCounter);
	event.set_object_id_size(COMP->concreteObjects.size());

	uint32_t index = 0;
	for (std::list<ConcreteObject>::const_iterator iter = COMP->concreteObjects.begin(); iter != COMP->concreteObjects.end(); iter++) {
		event.set_object_id(index, iter->getId());
		index++;
	}

	COMP->objectEventServer->put(event);
	std::cout << "[DetectionTask]" << " Sent " << event.get_object_id_size() << " object IDs by event." << std::endl;
}



void RackDetectionTask::setGrippableObjectsEpmty(){
	COMP->concreteObjects.clear();
	//COMP->environmentIdCounter++;
}

/*
 * Inserts the detected objects into the global list,
 * for publishing them via queries.
 */
void RackDetectionTask::setGrippableObjects(mrpt::poses::CPose3D rackPose){
	COMP->concreteObjects.clear();
	//COMP->environmentIdCounter++;

	//CommBasicObjects::CommPosition3d obj_dimensions(_obj_depth, _obj_width, _obj_height);
	CommBasicObjects::CommPosition3d obj_dimensions;
	obj_dimensions.setX(_obj_depth);
	obj_dimensions.setY(_obj_width);
	obj_dimensions.setZ(_obj_height);

	for (int i = 1; i <= _obj_types.size(); i++){
		ConcreteObject grippableObj;
		mrpt::poses::CPose3D surfacePose = calcObjectPose(rackPose, i);
		mrpt::poses::CPose3D centerPose = surfacePose + mrpt::poses::CPose3D(_obj_depth / 2, 0.0, 0.0, 0.0, 0.0, 0.0);

		grippableObj.setId(COMP->objectIdCounter);
		grippableObj.setPose(centerPose);
		grippableObj.setSurfacePose(surfacePose);
		grippableObj.setObjectClass(_obj_types[i - 1]);

		//setting dimensions is not necessary, since object
		//size is taken from openrave DB.
		grippableObj.setDimension(obj_dimensions);
		COMP->concreteObjects.push_back(grippableObj);
		COMP->objectIdCounter++;
	}
}


mrpt::poses::CPose3D RackDetectionTask::calcObjectPose(mrpt::poses::CPose3D rackPose, int rackLaneNo){
	float initial_x = COMP->getGlobalState().getGeneral().getInit_relative_obj_pose_x();
	float initial_y = COMP->getGlobalState().getGeneral().getInit_relative_obj_pose_y();
	float initial_z = COMP->getGlobalState().getGeneral().getInit_relative_obj_pose_z();

	float x = COMP->getGlobalState().getGeneral().getRelative_obj_pose_x();
	float y = COMP->getGlobalState().getGeneral().getRelative_obj_pose_y();
	float z = COMP->getGlobalState().getGeneral().getRelative_obj_pose_z();
	float yaw = 0.0;
	float pitch = COMP->getGlobalState().getGeneral().getRelative_obj_pose_pitch();
	float roll = 0.0;

//	if(COMP->getGlobalState().getGeneral().getPick_left()){
//		x = -1 * x * rackLaneNo;
//		yaw = 1.570796327;
//		std::cout<< "---> calc pose pick left " << std::endl;
//	}else {
//		y = y * rackLaneNo;
//		std::cout<< "---> calc pose pick front " << std::endl;
//	}

	x = initial_x + (x * (rackLaneNo -1));
	y = initial_y + (y * (rackLaneNo -1));
	z = initial_z + (z * (rackLaneNo -1));

	mrpt::poses::CPose3D relativeObjPose (x, y, z, yaw, pitch, roll);

	//it is expected, that the rack has the same angle like the
	//camera around x and y. But ICP could create wrong values. Hence
	//pitch and roll are set to zero.
	rackPose.setYawPitchRoll(rackPose.yaw(), 0.0, 0.0);
	mrpt::poses::CPose3D objPose = rackPose + relativeObjPose;

	return objPose;
}


/*
 * Sets the whole cloud without the detected objects as mesh to the concrete objects list.
 * This will be used for avoiding obstacles during path planning.
 */
void RackDetectionTask::setObstacleObject(pcl::PointCloud<pcl::PointXYZ>::Ptr obstacles_cloud){

	///////////////////////////////////////////
	// Create mesh (code from mapper.cc:915)

	pcl::ConcaveHull<pcl::PointXYZ> hr;
	hr.setAlpha (0.01);
	hr.setInputCloud (obstacles_cloud);

	pcl::PointCloud<pcl::PointXYZ>::Ptr hull_cloud(new pcl::PointCloud<pcl::PointXYZ>());
	std::vector<pcl::Vertices> vertices;
	//calculate mesh from point cloud
	std::cout << "Create mesh cloud..." << std::flush;
	hr.reconstruct (*hull_cloud, vertices);
	std::cout << " Done!" << std::endl;
	std::cout << "Mesh cloud size: " << hull_cloud->size() << std::endl;


	///////////////////////////////////////////
	// Convert mesh into transmission format

	//important info: modiefied pcl_typedefs.hh line 10 from pcl::PointXYZRGB PointT to  pcl::PointXYZ PointT
	PointCloud cloud_out;
	cloud_out.setPclPointCloud(hull_cloud);

	std::vector< std::vector <uint32_t> > vertices_out;
	vertices_out.resize(vertices.size ());

	uint32_t vertices_size = vertices[0].vertices.size();
	for (size_t i = 0; i < vertices.size (); ++i){
		vertices_out[i].resize(vertices_size);
		vertices_out[i][0] = vertices[i].vertices[0];
		vertices_out[i][1] = vertices[i].vertices[1];
		vertices_out[i][2] = vertices[i].vertices[2];
	}


	///////////////////////////////////////////
	// Prepare transmission object

	ConcreteObject obstacle_hull;
	CPose3D zero_pose;
	TPoint3D zero_point;

	obstacle_hull.setMinPoint(zero_point);
	obstacle_hull.setMaxPoint(zero_point);
	obstacle_hull.setPose(zero_pose);

	obstacle_hull.setId(COMP->objectIdCounter++);
	obstacle_hull.setObjectClass("OBSTACLE-HULL");

	ObjectBeliefs belief_hull;
	belief_hull.setBelief("OBSTACLE-HULL", 1.0);
	obstacle_hull.setBeliefs(belief_hull);

	obstacle_hull.setObstacleHullPointCloud(cloud_out);
	obstacle_hull.setObstacleHullvertices(vertices_out);

	COMP->concreteObjects.push_back(obstacle_hull);
}


/*
 * Returns the  pose of the rack (right corner of the shelf) with respect to
 * the robot (0,0,0). Calculation from transformation matrix.
*/
mrpt::poses::CPose3D RackDetectionTask::getRackPoseFromTransfMat(Eigen::Matrix<float, 4, 4> transform_mat){

	//cast transformation matrix into needed format
	const double transf_vals[16] = {transform_mat.coeff(0, 0), transform_mat.coeff(0, 1), transform_mat.coeff(0, 2), transform_mat.coeff(0, 3),
										  transform_mat.coeff(1, 0), transform_mat.coeff(1, 1), transform_mat.coeff(1, 2), transform_mat.coeff(1, 3),
			       	   	   	   	   	   	  transform_mat.coeff(2, 0), transform_mat.coeff(2, 1), transform_mat.coeff(2, 2), transform_mat.coeff(2, 3),
			       	   	   	   	   	   	  transform_mat.coeff(3, 0), transform_mat.coeff(3, 1), transform_mat.coeff(3, 2), transform_mat.coeff(3, 3)};
	mrpt::math::CMatrixDouble44 transform_mat_mrpt(transf_vals);

	std::cout << "transformation matrix:" << std::endl;
	std::cout << transform_mat_mrpt << std::endl;

	//create rack pose from transformation matrix
	mrpt::poses::CPose3D rackPose(transform_mat_mrpt);
	rackPose.setYawPitchRoll(rackPose.yaw(), rackPose.pitch(), rackPose.roll());


	return rackPose;
}


Eigen::Matrix<float, 4, 4> RackDetectionTask::doIcp(pcl::PointCloud<pcl::PointXYZ>::Ptr env_cloud, pcl::PointCloud<pcl::PointXYZ>::Ptr model_cloud){

	////////////////////
	//TEST
	  // Define a rotation matrix
	  Eigen::Affine3f transform_mat1 = Eigen::Affine3f::Identity();

	  // Define a translation of n meters on the x, y, z axis.
	  float transform_x = 0;
	  float transform_y = 0;
	  float transform_z = 0.0;

	  transform_mat1.translation() << transform_x, transform_y, transform_z;

	  float theta = _ptu_pan; // The angle of rotation in radians
	  transform_mat1.rotate (Eigen::AngleAxisf (theta, Eigen::Vector3f::UnitZ()));

	  std::cout<< "PTU (camera) pan angle: " << _ptu_pan << std::endl;

	///////////////////


	Eigen::Matrix<float, 4, 4> init_guess_mat;

	//initial guess on the left side
//	init_guess_mat <<  0.999988,   0.00336589, -0.00373381, 0.445273,
//					  -0.00351587, 0.999156,   -0.0409316,  0.539165,
//					   0.0035927,    0.040944,  0.999155,   0.805184,
//		               0,            0,         0,          1;


	init_guess_mat <<	transform_mat1(0, 0),   transform_mat1(0, 1), transform_mat1(0, 2), transform_mat1(0, 3),
			  	  	  	transform_mat1(1, 0),   transform_mat1(1, 1), transform_mat1(1, 2), transform_mat1(1, 3),
			  	  	  	transform_mat1(2, 0),   transform_mat1(2, 1), transform_mat1(2, 2), transform_mat1(2, 3),
			  	  	  	transform_mat1(3, 0),   transform_mat1(3, 1), transform_mat1(3, 2), transform_mat1(3, 3);


	std::cout << "Initial guess transformation:" << std::endl;
	std::cout << init_guess_mat << std::endl;

	pcl::IterativeClosestPoint<pcl::PointXYZ, pcl::PointXYZ> icp;
	icp.setInputSource(model_cloud);
	icp.setInputTarget(env_cloud);
	//icp.setEuclideanFitnessEpsilon()
	icp.setMaxCorrespondenceDistance(2);
	icp.setMaximumIterations(24);

	std::cout << "\n  Threshold: " << icp.getMaxCorrespondenceDistance() << std::endl;
	std::cout << "  Max. Iterations: " << icp.getMaximumIterations() << std::endl;

	pcl::PointCloud<pcl::PointXYZ> Final;
	mrpt::system::CTicTac  stopwatch;
	std::cout << "Running ICP..." << std::flush;
	stopwatch.Tic();
	//icp.align(Final);
	icp.align(Final, init_guess_mat);
	double icp_time = stopwatch.Tac();
	std::cout << "\nDone in " << icp_time << " s" << std::endl;

	std::cout << "has converged:" << icp.hasConverged() << " score: " << icp.getFitnessScore() << std::endl;
	Eigen::Matrix<float, 4, 4> transform_mat;
	if(icp.getFitnessScore() >  _max_detection_score ){
		//If score is too big the matching did not work well
		transform_mat << 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0;
	}else{
		std::cout << icp.getFinalTransformation() << std::endl;

		transform_mat = icp.getFinalTransformation();
	}

	pcl::PointCloud<pcl::PointXYZRGB>::Ptr show_cloud(new pcl::PointCloud<pcl::PointXYZRGB>);

	//shows environment cloud in result window
	for (size_t i = 0; i < env_cloud->points.size(); ++i){
		pcl::PointXYZRGB p(250, 0, 0);
		p.x = env_cloud->points[i].x;
		p.y = env_cloud->points[i].y;
		p.z = env_cloud->points[i].z;

		show_cloud->points.push_back(p);
	}

	//shows model cloud in result window
	for (size_t i = 0; i < model_cloud->points.size(); ++i) {
		pcl::PointXYZRGB p(0, 250, 0);
		p.x = model_cloud->points[i].x;
		p.y = model_cloud->points[i].y;
		p.z = model_cloud->points[i].z;

		show_cloud->points.push_back(p);
	}

	//shows shows transformed model cloud in result window
	for (size_t i = 0; i < Final.points.size(); ++i) {
		pcl::PointXYZRGB p(0, 0, 250);
		p.x = Final.points[i].x;
		p.y = Final.points[i].y;
		p.z = Final.points[i].z;

		show_cloud->points.push_back(p);
	}

	vHelper.showPointCloud(show_cloud, _cloud_window_index);

	return transform_mat;
}

void RackDetectionTask::findPickDirection(mrpt::poses::CPose3D sensor_pose){
	// yaw = azimuth (around z); pitch = elevation (around y); roll = roll (around x)
	//mrpt::poses::CPose3D camera_rot_pose(0, 0, 0, -1.5707963, 0, -1.5707963);
	//mrpt::poses::CPose3D ptu_pose = sensor_pose - camera_rot_pose;
	//Left:  ptu pose (sensor - camera): (x,y,z,yaw,pitch,roll)=(-68.0032, -1189.9339,  61.8444,    0.00deg, -88.45deg,  -36.09deg)
	//Front: ptu pose (sensor - camera): (x,y,z,yaw,pitch,roll)=(8.5368,   -1199.4718, 111.5977,    0.00deg,   1.50deg,  -27.50deg)
	//Right: ptu pose (sensor - camera): (x,y,z,yaw,pitch,roll)=(58.6053,  -1199.4718,  44.5158, -180.00deg,  88.55deg,  152.50deg)

	//TODO: camera rotation should come from somewhere else
	mrpt::poses::CPose3D camera_rot_pose(0, 0, 0, 0, -1.5707963, 1.5707963);
	mrpt::poses::CPose3D ptu_pose = sensor_pose + camera_rot_pose;
	// Left:  ptu pose (sensor - camera): (x,y,z,yaw,pitch,roll)=(61.5894,58.5900,1199.4718,88.45deg,27.50deg,-0.00deg)
	// Front: ptu pose (sensor - camera): (x,y,z,yaw,pitch,roll)=(111.5976,-8.5368,1199.4718,-1.50deg,27.50deg,-0.00deg)
	// Right: ptu pose (sensor - camera): (x,y,z,yaw,pitch,roll)=(44.5157,-58.6053,1199.4718,-91.45deg,27.50deg,-0.00deg)

	float ptu_pan = ptu_pose.yaw();
	std::cout << "PTU Pan: " << ptu_pan << std::endl;

	//Debug output
//	std::cout << "==> sensor pose: " << sensor_pose << std::endl;
//	std::cout << "==> camera rotation pose: " << camera_rot_pose << std::endl;
//	std::cout << "==> ptu pose (sensor - camera): " << ptu_pose << std::endl;

	//it is expected, that the robot stands parallel to the shelf.
	//Hence, the camera is defined to have 90 degree (front, left or right side)
	if(ptu_pan > 0.5){
		_ptu_pan = 1.5708; //object detection at the left side of the robot
		_pick_direction = left;
		std::cout << "Detected shelf on left side" << std::endl;
	}else if(ptu_pan < -0.5){
		_ptu_pan = -1.5708; //object detection at the right side of the robot
		_pick_direction = right;
		std::cout << "Detected shelf on right side" << std::endl;
	}else{
		_ptu_pan = 0.0; //object detection at the front of the robot
		_pick_direction = front;
		std::cout << "Detected shelf on front side. Setting " << std::endl;
	}
}


//Dirty way (not used)
void RackDetectionTask::findPickDirection(pcl::PointCloud<pcl::PointXYZ>::Ptr env_cloud){
	float min_x, max_x, min_y, max_y, min_z, max_z;

	pcl::PointCloud<pcl::PointXYZ>::Ptr tmp_cloud_ptr(new pcl::PointCloud<pcl::PointXYZ>);
	pcl::copyPointCloud(*env_cloud, *tmp_cloud_ptr);

	//cut out all points in front of the robot
	min_x = -1.0;
	max_x = 0.1;
	min_y = -1.0;
	max_y = 1.0;
	min_z = 0.1;
	max_z = 2.0;

	tmp_cloud_ptr = _pointManipulator.distanceFilterCloud(tmp_cloud_ptr, min_x, max_x, min_y, max_y, min_z, max_z);
	//std::cout << "===> tmp cloud ptr size: " << tmp_cloud_ptr->size()  << std::endl;
	//int modelCloudWindowIdx = vHelper.createPointCloudWindow("tmp Cloud");
	//vHelper.showPointCloud(tmp_cloud_ptr, modelCloudWindowIdx);

	if(tmp_cloud_ptr->size() < 1000){
		_ptu_pan = 0.0; //object detected at the front of the robot
		_pick_direction = front;
		std::cout << "===> Detected shelf on front side" << std::endl;
		return;
	}

	pcl::PointCloud<pcl::PointXYZ>::Ptr tmp_cloud_ptr2(new pcl::PointCloud<pcl::PointXYZ>);
	pcl::copyPointCloud(*env_cloud, *tmp_cloud_ptr2);

	//cut out all points on the left side of the robot
	min_x = -1.0;
	max_x = 1.0;
	min_y = -1.5;
	max_y = 0.0;
	min_z = 0.1;
	max_z = 2.0;

	tmp_cloud_ptr2 = _pointManipulator.distanceFilterCloud(tmp_cloud_ptr2, min_x, max_x, min_y, max_y, min_z, max_z);
	//std::cout << "===> tmp cloud ptr2 size: " << tmp_cloud_ptr2->size()  << std::endl;
	//int modelCloudWindowIdx2 = vHelper.createPointCloudWindow("tmp Cloud 2");
	//vHelper.showPointCloud(tmp_cloud_ptr2, modelCloudWindowIdx2);

	if(tmp_cloud_ptr2->size() < 1000){
		_ptu_pan = 1.5708; //object detected at the left side of the robot
		_pick_direction = left;
		std::cout << "===> Detected shelf on left side" << std::endl;
		return;
	}


	pcl::PointCloud<pcl::PointXYZ>::Ptr tmp_cloud_ptr3(new pcl::PointCloud<pcl::PointXYZ>);
	pcl::copyPointCloud(*env_cloud, *tmp_cloud_ptr3);

	//cut out all points on the left side of the robot
	min_x = -1.0;
	max_x = 1.0;
	min_y = 0.0;
	max_y = 1.5;
	min_z = 0.1;
	max_z = 2.0;

	tmp_cloud_ptr3 = _pointManipulator.distanceFilterCloud(tmp_cloud_ptr3, min_x, max_x, min_y, max_y, min_z, max_z);
	//std::cout << "===> tmp cloud ptr3 size: " << tmp_cloud_ptr3->size()  << std::endl;
	//int modelCloudWindowIdx3 = vHelper.createPointCloudWindow("tmp Cloud 3");
	//vHelper.showPointCloud(tmp_cloud_ptr3, modelCloudWindowIdx3);

	if(tmp_cloud_ptr3->size() < 1000){
		_ptu_pan = -1.5708; //object detected at the right side of the robot
		_pick_direction = right;
		std::cout << "===> Detected shelf on right side" << std::endl;
		return;
	}
}



/*
 * cut out rack from environment by filtering cloud in all directions.
 * -> shrink size for faster calculations and get rid of NaN values
*/
pcl::PointCloud<pcl::PointXYZ>::Ptr RackDetectionTask::cutOutRack(pcl::PointCloud<pcl::PointXYZ>::Ptr env_cloud){
	float min_x, max_x, min_y, max_y, min_z, max_z;

//	if (COMP->getGlobalState().getGeneral().getPick_left()) {
//		min_y = COMP->getGlobalState().getGeneral().getRack_dimension_min_x();
//		max_y = COMP->getGlobalState().getGeneral().getRack_dimension_max_x();
//		min_x = COMP->getGlobalState().getGeneral().getRack_dimension_min_y();
//		max_x = COMP->getGlobalState().getGeneral().getRack_dimension_max_y();
//		min_z = COMP->getGlobalState().getGeneral().getRack_dimension_min_z();
//		max_z = COMP->getGlobalState().getGeneral().getRack_dimension_max_z();
//	} else {
//		min_x = COMP->getGlobalState().getGeneral().getRack_dimension_min_x();
//		max_x = COMP->getGlobalState().getGeneral().getRack_dimension_max_x();
//		min_y = COMP->getGlobalState().getGeneral().getRack_dimension_min_y();
//		max_y = COMP->getGlobalState().getGeneral().getRack_dimension_max_y();
//		min_z = COMP->getGlobalState().getGeneral().getRack_dimension_min_z();
//		max_z = COMP->getGlobalState().getGeneral().getRack_dimension_max_z();
//	}


	if (_pick_direction == left) {
		min_y = COMP->getGlobalState().getGeneral().getRack_dimension_min_x();
		max_y = COMP->getGlobalState().getGeneral().getRack_dimension_max_x();
		min_x = COMP->getGlobalState().getGeneral().getRack_dimension_min_y();
		max_x = COMP->getGlobalState().getGeneral().getRack_dimension_max_y();
		min_z = COMP->getGlobalState().getGeneral().getRack_dimension_min_z();
		max_z = COMP->getGlobalState().getGeneral().getRack_dimension_max_z();
	} else if (_pick_direction == right) {
		min_y = COMP->getGlobalState().getGeneral().getRack_dimension_max_x() * -1;
		max_y = COMP->getGlobalState().getGeneral().getRack_dimension_min_x() * -1;
		min_x = COMP->getGlobalState().getGeneral().getRack_dimension_min_y();
		max_x = COMP->getGlobalState().getGeneral().getRack_dimension_max_y();
		min_z = COMP->getGlobalState().getGeneral().getRack_dimension_min_z();
		max_z = COMP->getGlobalState().getGeneral().getRack_dimension_max_z();
	} else if (_pick_direction == front) {
		min_x = COMP->getGlobalState().getGeneral().getRack_dimension_min_x();
		max_x = COMP->getGlobalState().getGeneral().getRack_dimension_max_x();
		min_y = COMP->getGlobalState().getGeneral().getRack_dimension_min_y();
		max_y = COMP->getGlobalState().getGeneral().getRack_dimension_max_y();
		min_z = COMP->getGlobalState().getGeneral().getRack_dimension_min_z();
		max_z = COMP->getGlobalState().getGeneral().getRack_dimension_max_z();
	}


	std::cout << "min x: " << min_x << " , max x: " << max_x << std::endl;
	std::cout << "min y: " << min_y << " , max y: " << max_y << std::endl;
	std::cout << "min z: " << min_z << " , max z: " << max_z << std::endl;

	return _pointManipulator.distanceFilterCloud(env_cloud, min_x, max_x, min_y, max_y, min_z, max_z);
}



//delete objects from cloud, for better path planning
pcl::PointCloud<pcl::PointXYZ>::Ptr RackDetectionTask::cutOutObjects(pcl::PointCloud<pcl::PointXYZ>::Ptr env_cloud){

	float obj_height = _obj_height;
	float obj_width = _obj_width;
	float obj_depth = _obj_depth;
	float deviation = 0.01; // additional 1 cm
	std::vector<float> obj_dimensions_xyz;

	if (_pick_direction == left || _pick_direction == right) {
		obj_dimensions_xyz.push_back(obj_width + deviation);
		obj_dimensions_xyz.push_back(obj_depth + deviation * 4);
		// changed from *10 to *1 again
		obj_dimensions_xyz.push_back(obj_height + deviation* 1);
	} else if (_pick_direction == front){
		obj_dimensions_xyz.push_back(obj_depth + deviation * 4);
		obj_dimensions_xyz.push_back(obj_width + deviation);
        // changed from *10 to *1 again
		obj_dimensions_xyz.push_back(obj_height + deviation* 1);
	}

	for (std::list<ConcreteObject>::const_iterator iter = COMP->concreteObjects.begin(); iter != COMP->concreteObjects.end(); iter++) {
		std::vector<float> obj_center_xyz;
		obj_center_xyz.push_back(iter->getSurfacePose().x());
		obj_center_xyz.push_back(iter->getSurfacePose().y());
		obj_center_xyz.push_back(iter->getSurfacePose().z());

		mrpt::poses::CPose3D center_pose(iter->getPose().x(), iter->getPose().y(), iter->getPose().z(), iter->getPose().yaw(), iter->getPose().pitch(), iter->getPose().roll());

		env_cloud = _pointManipulator.removeObjectFromPC(env_cloud, obj_center_xyz, obj_dimensions_xyz, center_pose);
	}
	return env_cloud;
}


pcl::PointCloud<pcl::PointXYZ>::Ptr RackDetectionTask::loadRackModel(std::string filePath){
	  pcl::PointCloud<pcl::PointXYZ>::Ptr model_cloud_ptr (new pcl::PointCloud<pcl::PointXYZ>);

	  if (pcl::io::loadPCDFile<pcl::PointXYZ> (filePath, *model_cloud_ptr) == -1) //* load model file
	  {
	    std::cout << "Couldn't read model file." << std::endl;
	  }

	  return model_cloud_ptr;

}


pcl::PointCloud<pcl::PointXYZ>::Ptr RackDetectionTask::downsampleCloud (pcl::PointCloud<pcl::PointXYZ>::Ptr cloud){
	pcl::PointCloud<pcl::PointXYZ>::Ptr downsampled_cloud (new pcl::PointCloud<pcl::PointXYZ>);
	pcl::VoxelGrid<pcl::PointXYZ> voxel_grid;
	// define filter leaf size 1cm
	float leaf_size = 0.01;

	voxel_grid.setInputCloud(cloud);
	voxel_grid.setLeafSize (leaf_size, leaf_size, leaf_size);

	voxel_grid.filter (*downsampled_cloud);

	return downsampled_cloud;
}


//=============================================
// RACK MODEL CREATION

void RackDetectionTask::createRackModelCloud (pcl::PointCloud<pcl::PointXYZ>::Ptr cloud){
	//not needed since rack is already cut out before
//	pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_filtered_x (new pcl::PointCloud<pcl::PointXYZ>);
//	pcl::PassThrough<pcl::PointXYZ> pass_x;
//	pass_x.setInputCloud(cloud);
//	pass_x.setFilterFieldName("x");
//	pass_x.setFilterLimits(-0.0, 2.0);
//	std::cout << "Distance filter x cloud..." << std::flush;
//	pass_x.filter(*cloud_filtered_x);
//	std::cout << " Done!" << std::endl;
//	std::cout << "Cloud size: " << cloud_filtered_x->size() << std::endl;
//
//
//	pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_filtered_y(new pcl::PointCloud<pcl::PointXYZ>);
//	pcl::PassThrough<pcl::PointXYZ> pass_y;
//	pass_y.setInputCloud(cloud_filtered_x);
//	pass_y.setFilterFieldName("y");
//	pass_y.setFilterLimits(-0.5, 0.5);
//	std::cout << "Distance filter y cloud..." << std::flush;
//	pass_y.filter(*cloud_filtered_y);
//	std::cout << " Done!" << std::endl;
//	std::cout << "Cloud size: " << cloud_filtered_y->size() << std::endl;
//
//	pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_filtered_z(new pcl::PointCloud<pcl::PointXYZ>);
//	pcl::PassThrough<pcl::PointXYZ> pass_z;
//	pass_z.setInputCloud(cloud_filtered_y);
//	pass_z.setFilterFieldName("z");
//	pass_z.setFilterLimits(-0.3, 2.5);
//	std::cout << "Distance filter y cloud..." << std::flush;
//	pass_z.filter(*cloud_filtered_z);
//	std::cout << " Done!" << std::endl;
//	std::cout << "Cloud size: " << cloud_filtered_z->size() << std::endl;

	//if do not use current camera capture load model from file
	if(!COMP->getGlobalState().getModelCreation().getCapture_model()){
	// Load file from filesystem | Works with PCD and PLY files
	  cloud = pcl::PointCloud<pcl::PointXYZ>::Ptr(new pcl::PointCloud<pcl::PointXYZ> ());
	  if (pcl::io::loadPCDFile(COMP->getGlobalState().getModelCreation().getLoad_model_path(), *cloud) < 0) {
		std::cout << "Error loading point cloud " << COMP->getGlobalState().getModelCreation().getLoad_model_path()	<< std::endl << std::endl;
		return;
	  }
		//debug code shows loaded model cloud
		int loadedCloudWindowIdx = vHelper.createPointCloudWindow("Loaded Model Cloud");
		vHelper.showPointCloud(cloud, loadedCloudWindowIdx);
	}

	pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_cut_out = cutOutRack(cloud);

	pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_downsampled = downsampleCloud(cloud_cut_out);

	pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_transformed = transformCloud(cloud_downsampled);

	//save model to file
	pcl::io::savePCDFileASCII(_model_file_save_path, *cloud_transformed);
	std::cerr << "Saved " << cloud_transformed->points.size() << " data points to file." << std::endl;

	//debug code shows created model cloud
	int modelCloudWindowIdx = vHelper.createPointCloudWindow("New Model Cloud");
	vHelper.showPointCloud(cloud_transformed, modelCloudWindowIdx);
}

/*
 * This function is needed for transforming the model cloud to the
 * origin of the robot coordinate frame.
 * see http://pointclouds.org/documentation/tutorials/matrix_transform.php
 */
pcl::PointCloud<pcl::PointXYZ>::Ptr RackDetectionTask::transformCloud(pcl::PointCloud<pcl::PointXYZ>::Ptr source_cloud){

	pcl::PointCloud<pcl::PointXYZ>::Ptr filtered_cloud (new pcl::PointCloud<pcl::PointXYZ>);

  //Delete flying pixels
  pcl::RadiusOutlierRemoval<pcl::PointXYZ> outrem;
  outrem.setInputCloud(source_cloud);
  outrem.setRadiusSearch(0.05);
  outrem.setMinNeighborsInRadius(12);
  outrem.filter (*filtered_cloud);


  /* Reminder: how transformation matrices work :

            |-------> This column is the translation
    | 1 0 0 x |  \
    | 0 1 0 y |   }-> The identity 3x3 matrix (no rotation) on the left
    | 0 0 1 z |  /
    | 0 0 0 1 |    -> We do not use this line (and it has to stay 0,0,0,1)

  */

  // Define a rotation matrix
  Eigen::Affine3f transform_mat = Eigen::Affine3f::Identity();

  // Define a translation of n meters on the x, y, z axis.
  float transform_x = COMP->getGlobalState().getModelCreation().getTransformation_x();
  float transform_y = COMP->getGlobalState().getModelCreation().getTransformation_y();
  float transform_z = COMP->getGlobalState().getModelCreation().getTransformation_z();

  transform_mat.translation() << transform_x, transform_y, transform_z;

  //float theta = M_PI/4; // The angle of rotation in radians
  float theta = COMP->getGlobalState().getModelCreation().getTheta(); // The angle of rotation in radians
  transform_mat.rotate (Eigen::AngleAxisf (theta, Eigen::Vector3f::UnitZ()));

  // Print the transformation
  printf ("\ntransformation matrix: \n");
  std::cout << transform_mat.matrix() << std::endl;

  // Executing the transformation
  pcl::PointCloud<pcl::PointXYZ>::Ptr transformed_cloud (new pcl::PointCloud<pcl::PointXYZ> ());
  pcl::transformPointCloud (*filtered_cloud, *transformed_cloud, transform_mat);

  //save model to file
  //pcl::io::savePCDFileASCII(_model_file_save_path, *transformed_cloud);
  //std::cerr << "Saved " << transformed_cloud->points.size() << " data points to file." << std::endl;

  return transformed_cloud;
}
